{"cells":[{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom functools import wraps\nfrom tensorflow import keras\nfrom sklearn.datasets import load_iris\nfrom tensorflow.keras import layers\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\n\n#Importiere MINST Daten\n#from tensorflow.examples.tutorials.mnist import input_data\n#mnist = input_data.read_data_sets('data/', one_hot=True)\n\n#mint=tf.keras.datasets.mnist\n#(x_,y_),(x_1,y_1)=mnist.load_data()\n\nfrom tensorflow.keras.datasets import mnist\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Funktionen generieren und Accuracy auswerten"},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = keras.Input(shape=(784,), name=\"digits\")\nx = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\nx = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\noutputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n# Verarbeiten der Daten\nx_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\nx_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n\ny_train = y_train.astype(\"float32\")\ny_test = y_test.astype(\"float32\")\n\n# Reservieren 10.000 Samples zur Validierung\nx_val = x_train[-10000:]\ny_val = y_train[-10000:]\nx_train = x_train[:-10000]\ny_train = y_train[:-10000]","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n    optimizer=keras.optimizers.RMSprop(),  # Optimizer\n    # Loss function to minimize\n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    # List of metrics to monitor\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Fit model on training data\")\nhistory = model.fit(\n    x_train,\n    y_train,\n    batch_size=64,\n    epochs=2,\n    # We pass some validation for\n    # monitoring validation loss and metrics\n    # at the end of each epoch\n    validation_data=(x_val, y_val),\n)","execution_count":11,"outputs":[{"output_type":"stream","text":"Fit model on training data\nEpoch 1/2\n782/782 [==============================] - 4s 6ms/step - loss: 0.0722 - sparse_categorical_accuracy: 0.9785 - val_loss: 0.1029 - val_sparse_categorical_accuracy: 0.9697\nEpoch 2/2\n782/782 [==============================] - 4s 5ms/step - loss: 0.0599 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.0947 - val_sparse_categorical_accuracy: 0.9725\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_logger(orig_func):\n    import logging\n    logging.basicConfig(filename='Testdatenfile.log'.format(orig_func.__name__), level=logging.INFO)\n\n    @wraps(orig_func)\n    def wrapper(*args):\n        #logging.info('Ran with args: {}'.format(args))\n        logging.info(orig_func(*args))\n        return orig_func(*args)\n\n    return wrapper\n\ndef my_timer(orig_func):\n    import time\n    import logging\n    logging.basicConfig(filename='Testdatenfile.log'.format(orig_func.__name__), level=logging.INFO)\n\n    @wraps(orig_func)\n    def wrapper(*args):\n        t1 = time.time()\n        result = orig_func(*args)\n        t2 = time.time() - t1\n        print('{} ran in: {} sec'.format(orig_func.__name__, t2))\n        logging.info('{} ran in: {} sec'.format(orig_func.__name__, t2))\n        return result\n\n    return wrapper","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@my_logger\n@my_timer\ndef fit(x_train, y_train): \n    return model.fit(x_train, y_train)\n    \n@my_logger\n@my_timer\ndef predict(x_test):\n    return model.predict(x_test)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(fit(x_train, y_train))\n\nprint(predict(x_test))","execution_count":15,"outputs":[{"output_type":"stream","text":"1563/1563 [==============================] - 6s 4ms/step - loss: 0.0603 - sparse_categorical_accuracy: 0.9821\nfit ran in: 6.685071706771851 sec\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.0533 - sparse_categorical_accuracy: 0.9843\nfit ran in: 6.5847601890563965 sec\n<tensorflow.python.keras.callbacks.History object at 0x7fb04c48f3d0>\npredict ran in: 0.6255404949188232 sec\npredict ran in: 0.5951237678527832 sec\n[[1.27383489e-14 1.19200275e-11 2.35281359e-08 ... 9.99998450e-01\n  3.81870143e-12 2.15759655e-09]\n [2.15352303e-15 1.04889009e-09 1.00000000e+00 ... 3.11272347e-15\n  3.59436900e-13 5.49058709e-19]\n [1.11915730e-08 9.99228597e-01 3.06771108e-05 ... 5.35879808e-04\n  1.26411454e-04 9.25061101e-07]\n ...\n [2.65568194e-13 1.19467871e-13 2.91245045e-12 ... 3.90618510e-07\n  1.95280847e-07 1.36016679e-04]\n [2.58637716e-19 2.91406518e-16 1.85969723e-17 ... 4.95089021e-15\n  5.11160891e-10 1.19346553e-14]\n [3.20318585e-16 2.90695648e-18 4.95527881e-13 ... 1.00524975e-18\n  8.15504348e-15 9.68625072e-20]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Gut gemacht!"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}